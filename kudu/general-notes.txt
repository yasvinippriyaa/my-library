Tablet Server
A tablet server stores and serves tablets to clients. One tablet server can serve multiple tablets, and one tablet can be served by multiple tablet servers (by replication, not in bits).
Tablet Servers:
TS-1: tablet_A (leader), tablet_B (follower)
TS-2: tablet_A (follower), tablet_C (leader)
TS-3: tablet_A (follower), tablet_B (leader)

Master
The master keeps track of all the tablets, tablet servers, the Catalog Table, and other metadata related to the cluster.
The master writes the metadata for the new table into the catalog table, and coordinates the process of creating tablets on the tablet servers.
All the master’s data is stored in a tablet, which can be replicated to all the other candidate masters.

Catalog Table
table schemas, locations, and states
the list of existing tablets, which tablet servers have replicas of each tablet, the tablet’s current state, and start and end keys.

Logical Replication
Kudu replicates operations, not on-disk data. This is referred to as logical replication, as opposed to physical replication.

*** Kudu offers the powerful combination of fast inserts and updates with efficient columnar scans to enable real-time analytics use cases on a single storage layer.

Operating System Requirements
A kernel and filesystem that support hole punching.
Optional: If support for Kudu’s NVM (non-volatile memory) block cache is desired, install the memkind library.

Configuring Kudu
Kudu relies on timestamps generated by its clock implementation for the MVCC and for providing consistency guarantees when processing write and read requests.
hybrid time: is a combination of the node’s system clock and a Lamport clock.
*** (check which clock is being used) Using the HybridClock implementation is a must for any production-grade, POC, and other regular Kudu deployments: that’s why --use_hybrid_clock is set true by default. Setting the flag to false makes Kudu servers use the LogicalClock implementation: running with such a clock implementation is acceptable only in the context of running specifically crafted test scenarios in Kudu development environment.
*** To provide better accuracy for multi-node cluster deployments where each node maintains its own system clock, the HybridClock implementation requires each node’s system clock to be synchronized by NTP.
*** (check whether this is set/unset) Setting --time_source=system_unsync removes the requirement for the node’s system clock to be synchronized by NTP — this allows users to run test clusters on a single node where there is only one clock used by all Kudu servers. Setting --time_source=system_unsync is strongly discouraged in any multi-node Kudu cluster, unless system clocks of all Kudu nodes are guaranteed to always be synchronized with each other. 
*** If deploying a Kudu cluster in AWS/EC2 or GCE/GCP public clouds, it might make sense to set --time_source=auto for all Kudu masters and tablet servers in the cluster.
*** Conf TS --memory_limit_hard_bytes default:4294967296 Maximum amount of memory a Tablet Server can consume before it starts rejecting all incoming writes.Maximum amount of memory a Tablet Server can consume before it starts rejecting all incoming writes.
Kudu provides no additional tooling to create or drop Hive databases. Administrators or users should use existing Hive tools such as the Beeline Shell or Impala to do so.

https://kudu.apache.org/docs/hive_metastore.html#metadata_sync
https://kudu.apache.org/docs/kudu_impala_integration.html#_optimizing_performance_for_evaluating_sql_predicates
https://kudu.apache.org/docs/kudu_impala_integration.html#partitioning_rules_of_thumb
*** For large tables, such as fact tables, aim for as many tablets as you have cores in the cluster. In general, be mindful the number of tablets limits the parallelism of reads, in the current implementation. Increasing the number of tablets significantly beyond the number of cores is likely to have diminishing returns.

*** Inserting in bulk:
Multiple single INSERT statements - likely to be inefficient, This will lead to relatively high latency and poor throughput.
Single INSERT statement with multiple VALUES - check batch_size, This approach may perform slightly better than multiple sequential INSERT statements by amortizing the query start-up penalties on the Impala side.Increasing the Impala batch size causes Impala to use more memory. You should verify the impact on your cluster and tune accordingly.

*** != and LIKE predicates are not pushed to Kudu, and instead will be evaluated by the Impala scan node. This may decrease performance relative to other types of predicates.
!= and LIKE predicates are not pushed to Kudu, and instead will be evaluated by the Impala scan node. This may decrease performance relative to other types of predicates.

*** Known Limitations in Schema Design https://kudu.apache.org/docs/schema_design.html#known-limitations
